{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1x7oPVkgfzX2Cpq6T4CkGi7dDgHFRj9ja","authorship_tag":"ABX9TyP7q3ux1YmeAkqXAlxZXFzL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Goal**: Map character spans â†’ token-level BIO labels using the same HF tokenizer you'll use for BioBERT. Create small manual validation set selection plan."],"metadata":{"id":"t1_gt2Imi8r-"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P_Ujc-7Bix0t","executionInfo":{"status":"ok","timestamp":1760895438809,"user_tz":-480,"elapsed":11367,"user":{"displayName":"SOLOMON CHAN KAY PONG _","userId":"04107635501287651322"}},"outputId":"97b2ec21-d64a-4cb2-f398-c50420be1ed8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"]}],"source":["!pip install transformers datasets"]},{"cell_type":"code","source":["import json, random, os\n","from transformers import AutoTokenizer\n","\n","# Paths (adjust to your Drive paths)\n","DRIVE_ROOT = \"/content/drive/MyDrive/Colab Notebooks/CS425 GenAI for NLC\"\n","os.makedirs(DRIVE_ROOT, exist_ok=True)\n","\n","POS_JSONL = f\"{DRIVE_ROOT}/auto_labeled_sample.jsonl\"      # positives with spans\n","NEG_JSONL = f\"{DRIVE_ROOT}/unknown_spans.jsonl\"            # negatives, no spans\n","OUT_JSONL = f\"{DRIVE_ROOT}/train.jsonl\"                    # HF Datasets JSONL\n","OUT_CONLL = f\"{DRIVE_ROOT}/train_conll.txt\"                # CoNLL export\n","\n","# Tokenizer (same as for BioBERT fine-tune)\n","tok = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\", use_fast=True)\n","\n","def charspan_to_bio(text, spans, tokenizer):\n","    enc = tokenizer(\n","        text,\n","        return_offsets_mapping=True,\n","        add_special_tokens=False,\n","        truncation=False\n","    )\n","    offsets = enc[\"offset_mapping\"]\n","    tokens = tokenizer.convert_ids_to_tokens(enc[\"input_ids\"])\n","    tags = [\"O\"] * len(tokens)\n","\n","    # Normalize spans: accept (start,end,label) or (start,end,label,conf)\n","    norm_spans = []\n","    for sp in spans:\n","        if len(sp) >= 3:\n","            norm_spans.append((int(sp[0]), int(sp[1]), str(sp[2]).strip().lower()))\n","    # Assign BIO\n","    for (s, e, _label) in norm_spans:\n","        started = False\n","        for i, (ts, te) in enumerate(offsets):\n","            if ts >= s and te <= e:\n","                tags[i] = \"B-SYM\" if not started else \"I-SYM\"\n","                started = True\n","    return tokens, tags\n","\n","def iter_jsonl(path):\n","    with open(path, \"r\", encoding=\"utf-8\") as f:\n","        for line in f:\n","            if line.strip():\n","                yield json.loads(line)\n","\n","# Load positives (with spans)\n","pos_examples = list(iter_jsonl(POS_JSONL))\n","\n","# Load negatives; sample to target ratio (1:1 with positives or as needed)\n","neg_examples = []\n","if os.path.exists(NEG_JSONL):\n","    neg_all = list(iter_jsonl(NEG_JSONL))\n","    random.shuffle(neg_all)\n","    neg_examples = neg_all[:len(pos_examples)]\n","\n","# Convert and write JSONL + CoNLL\n","with open(OUT_JSONL, \"w\", encoding=\"utf-8\") as jout, open(OUT_CONLL, \"w\", encoding=\"utf-8\") as cfile:\n","    # Positives\n","    for ex in pos_examples:\n","        tokens, tags = charspan_to_bio(ex[\"text\"], ex.get(\"spans\", []), tok)\n","        json.dump({\"text\": ex[\"text\"], \"tokens\": tokens, \"tags\": tags}, jout, ensure_ascii=False)\n","        jout.write(\"\\n\")\n","        for t, y in zip(tokens, tags):\n","            cfile.write(f\"{t} {y}\\n\")\n","        cfile.write(\"\\n\")\n","    # Negatives (all O)\n","    for ex in neg_examples:\n","        tokens, tags = charspan_to_bio(ex[\"text\"], [], tok)\n","        json.dump({\"text\": ex[\"text\"], \"tokens\": tokens, \"tags\": tags}, jout, ensure_ascii=False)\n","        jout.write(\"\\n\")\n","        for t, y in zip(tokens, tags):\n","            cfile.write(f\"{t} {y}\\n\")\n","        cfile.write(\"\\n\")\n","\n","print(\"Wrote:\", OUT_JSONL, \"and\", OUT_CONLL)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ew7XM1e3i8Vs","executionInfo":{"status":"ok","timestamp":1760898151167,"user_tz":-480,"elapsed":62058,"user":{"displayName":"SOLOMON CHAN KAY PONG _","userId":"04107635501287651322"}},"outputId":"a610acbf-32a6-41f5-c84c-84e9b2276b89"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Wrote: /content/drive/MyDrive/Colab Notebooks/CS425 GenAI for NLC/train.jsonl and /content/drive/MyDrive/Colab Notebooks/CS425 GenAI for NLC/train_conll.txt\n"]}]},{"cell_type":"markdown","source":["# Select 150 diverse sentences for manual validation:\n","*   After BIO conversion, randomly sample 150 sentences from the combined set.\n","*   Stratify by symptom type (use label counts from your silver set) and include ambiguous/edge cases.\n","*   Save to data/validation_sample.jsonl.\n"],"metadata":{"id":"zSe1dC1fngss"}},{"cell_type":"code","source":["import random, json\n","\n","def stratified_sample(bio_file, n=150):\n","    with open(bio_file) as f:\n","        data = [json.loads(line) for line in f]\n","    # Optionally stratify by label type\n","    random.shuffle(data)\n","    sample = data[:n]\n","    with open('data/validation_sample.jsonl', 'w') as out:\n","        for entry in sample:\n","            json.dump(entry, out)\n","            out.write('\\n')"],"metadata":{"id":"KGnDbYpmn9e8","executionInfo":{"status":"ok","timestamp":1760898163350,"user_tz":-480,"elapsed":45,"user":{"displayName":"SOLOMON CHAN KAY PONG _","userId":"04107635501287651322"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# Save outputs, tokenizer, and checkpoints to Drive and locally\n","Save BIO files, tokenizer configs, and later NER checkpoints to your Drive path to persist across Colab runtimes, and download locally via files.download or by zipping and exporting.â€‹"],"metadata":{"id":"KwUmIWx8qBMK"}},{"cell_type":"code","source":["# Save tokenizer snapshot used for conversion (ensures later consistency)\n","tok.save_pretrained(f\"{DRIVE_ROOT}/tokenizer_biobert_v1.1\")\n","\n","# Optional: zip outputs to download locally from Colab\n","!zip -r /content/bio_outputs.zip \"$DRIVE_ROOT\"\n","from google.colab import files\n","files.download(\"/content/bio_outputs.zip\")\n"],"metadata":{"id":"HdtddJ6OjTwD"},"execution_count":null,"outputs":[]}]}